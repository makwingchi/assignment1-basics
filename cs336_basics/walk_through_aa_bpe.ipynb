{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d0e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670952b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import regex as re\n",
    "import multiprocessing\n",
    "from copy import deepcopy\n",
    "from typing import BinaryIO, List, Tuple, Iterable\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2968ca1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3abfb76f",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE) Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d834dab",
   "metadata": {},
   "source": [
    "## The Unicode Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ddb9e",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ebbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9020c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeac295",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668dff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6020594e",
   "metadata": {},
   "source": [
    "## Unicode Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"hello! こんにちは!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf8_encoded = test_string.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f263f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the byte values for the encoded string (integers from 0 to 255).\n",
    "list(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd872924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One byte does not necessarily correspond to one Unicode character!\n",
    "print(len(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447129bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55387b8c",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849a97da",
   "metadata": {},
   "source": [
    "#### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_string.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_string.encode(\"utf-8\")).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_string.encode(\"utf-16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbaff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_string.encode(\"utf-16\")).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a071cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_string.encode(\"utf-32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c597644",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_string.encode(\"utf-32\")).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a2d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6c5624",
   "metadata": {},
   "source": [
    "#### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes): \n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbba824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"hello 你好\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in \"hello 你好\".encode(\"utf-8\"):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70871439",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello 你好\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes([104]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes([228, 189, 160]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes([228]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e84e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54e4b6a7",
   "metadata": {},
   "source": [
    "#### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes([228, 189]).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454ec0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d25f9cef",
   "metadata": {},
   "source": [
    "## Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(\"the\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651c231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164deeb8",
   "metadata": {},
   "source": [
    "## BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95054cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in re.finditer(PAT, \"some text that i'll pre-tokenize\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d036974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47c45c63",
   "metadata": {},
   "source": [
    "### Problem (train_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data1/ds/scratch/assignment1-basics/data/TinyStoriesV2-GPT4-valid.txt\", \"rb\") as f:\n",
    "    num_processes = 4\n",
    "    boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "    # The following is a serial implementation, but you can parallelize this\n",
    "    # by sending each start/end pair to a set of processes.\n",
    "    for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        # Run pre-tokenization on your chunk and store the counts for each pre-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c823b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_pat = re.escape(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = re.split(\"|\".join([eos_pat]), chunk)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc98d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = splits[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tokens = re.findall(PAT, test_doc)\n",
    "byte_tokens = [s.encode(\"utf-8\") for s in str_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_bytes = [\n",
    "    token.encode(\"utf-8\") for token in [\"<|endoftext|>\", \"<|startoftext|>\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e0738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_naive(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str]\n",
    "):\n",
    "    GPT2_REGEX_PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    idx2bytes = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    idx = 256\n",
    "\n",
    "    # ------ 1. deal with special tokens\n",
    "    special_token_patterns = []\n",
    "    for special_token in special_tokens:\n",
    "        encoded_special_token = special_token.encode(\"utf-8\")\n",
    "        idx2bytes[idx] = encoded_special_token\n",
    "        idx += 1\n",
    "\n",
    "        special_token_patterns.append(re.escape(special_token))\n",
    "    \n",
    "    if len(idx2bytes) > vocab_size:\n",
    "        raise ValueError(f\"desired vocabulary size of {vocab_size} is smaller than initial vocabulary size of {len(idx2bytes)}\")\n",
    "    \n",
    "    # ------ 2. read text\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # ------ 3. initialize byte_tokens_cnt and byte_token_to_word mappings\n",
    "    splits = re.split(\"|\".join(special_token_patterns), text)\n",
    "    print(splits)\n",
    "    byte_tokens_cnt = defaultdict(int) # {(b'l', b'o', b'w'): 3, (b'l', b'o', b'b'): 4, ...}\n",
    "\n",
    "    for split in splits:\n",
    "        # doc = split.strip()\n",
    "\n",
    "        str_tokens = re.findall(GPT2_REGEX_PAT, split)\n",
    "        for s in str_tokens:\n",
    "            encoded_s = list(s.encode(\"utf-8\")) # [111,222,123,...]\n",
    "            byte_s = tuple([bytes([e]) for e in encoded_s])\n",
    "            byte_tokens_cnt[byte_s] += 1\n",
    "\n",
    "    # ------ 4. perform merges\n",
    "    merges = []\n",
    "    while len(idx2bytes) < vocab_size:\n",
    "        # ------ 4.1 perform pair counts\n",
    "        pair_cnt = defaultdict(int) # {(b'l', b'o'): 7, (b'o', b'w'): 3, ...}\n",
    "        for k, v in byte_tokens_cnt.items():\n",
    "            word_len = len(k)\n",
    "            for i in range(word_len-1):\n",
    "                curr_pair = (k[i], k[i+1])\n",
    "                pair_cnt[curr_pair] += v\n",
    "        \n",
    "        if len(pair_cnt) == 0:\n",
    "            break\n",
    "        \n",
    "        # print(pair_cnt)\n",
    "        # ------ 4.2 get candidate with max pair count\n",
    "        max_cnt = max(pair_cnt.values())\n",
    "        candidates = [k for k, v in pair_cnt.items() if v==max_cnt]\n",
    "        l, r = max(candidates)\n",
    "        curr_merge = l + r\n",
    "        merges.append((l, r))\n",
    "\n",
    "        # ------ 4.3 update byte_tokens_cnt\n",
    "        new_byte_tokens_cnt = {}\n",
    "        for k, v in byte_tokens_cnt.items():\n",
    "            word_len = len(k)\n",
    "            i = 0\n",
    "            curr_byte_tokens = []\n",
    "\n",
    "            while i < word_len:\n",
    "                curr_l = k[i]\n",
    "                if i < word_len - 1:\n",
    "                    curr_r = k[i+1]\n",
    "                else:\n",
    "                    curr_r = None\n",
    "\n",
    "                if curr_l == l and curr_r == r:\n",
    "                    curr_byte_tokens.append(l+r)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    curr_byte_tokens.append(curr_l)\n",
    "                    i += 1\n",
    "            \n",
    "            new_byte_tokens_cnt[tuple(curr_byte_tokens)] = v\n",
    "        \n",
    "        byte_tokens_cnt = new_byte_tokens_cnt\n",
    "        # print(byte_tokens_cnt)\n",
    "        \n",
    "        # ------ 4.4 update idx2bytes\n",
    "        idx2bytes[idx] = curr_merge\n",
    "        idx += 1\n",
    "    \n",
    "    return idx2bytes, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1, merges1 = train_bpe_naive(\n",
    "    \"/data1/ds/scratch/assignment1-basics/data/simple_test.txt\",\n",
    "    10000,\n",
    "    [\"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cca7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ae1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(input_path, start, end, special_token_patterns):\n",
    "    GPT2_REGEX_PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    splits = re.split(\"|\".join(special_token_patterns), chunk)\n",
    "    # print(splits)\n",
    "    byte_tokens_cnt = defaultdict(int) # {(b'l', b'o', b'w'): 3, (b'l', b'o', b'b'): 4, ...}\n",
    "\n",
    "    for split in splits:\n",
    "        str_tokens = re.findall(GPT2_REGEX_PAT, split)\n",
    "\n",
    "        for s in str_tokens:\n",
    "            encoded_s = list(s.encode(\"utf-8\")) # [111,222,123,...]\n",
    "            byte_s = tuple([bytes([e]) for e in encoded_s])\n",
    "            byte_tokens_cnt[byte_s] += 1\n",
    "    \n",
    "    return byte_tokens_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539dd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_improved(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    special_tokens: List[str]\n",
    "):\n",
    "    idx2bytes = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    idx = 256\n",
    "\n",
    "    # ------ 1. deal with special tokens\n",
    "    special_token_patterns = []\n",
    "    for special_token in special_tokens:\n",
    "        encoded_special_token = special_token.encode(\"utf-8\")\n",
    "        idx2bytes[idx] = encoded_special_token\n",
    "        idx += 1\n",
    "\n",
    "        special_token_patterns.append(re.escape(special_token))\n",
    "    \n",
    "    if len(idx2bytes) > vocab_size:\n",
    "        raise ValueError(f\"desired vocabulary size of {vocab_size} is smaller than initial vocabulary size of {len(idx2bytes)}\")\n",
    "    \n",
    "    # ------ 2. read text\n",
    "    # with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    #     text = f.read()\n",
    "\n",
    "    args = []\n",
    "\n",
    "    # copy from pretokenization_example.py\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        num_processes = 4\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            args.append((input_path, start, end, special_token_patterns))\n",
    "\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        results = pool.starmap(pretokenize, args)\n",
    "    \n",
    "    # ------ 3. initialize byte_tokens_cnt\n",
    "    byte_tokens_cnt = defaultdict(int)\n",
    "    for _map in results:\n",
    "        for k, v in _map.items():\n",
    "            byte_tokens_cnt[k] += v\n",
    "\n",
    "    # ------ 4. prepare pair_cnt\n",
    "    pair_cnt = defaultdict(int) # {(b'l', b'o'): 7, (b'o', b'w'): 3, ...}\n",
    "    pair2keys = defaultdict(set) # {(b'o', b'w'): { (b'l', b'o', b'w', b'e', b'r'), (b'p', b'o', b'w', b'e', b'r'), ... }\n",
    "    for k, v in byte_tokens_cnt.items():\n",
    "        word_len = len(k)\n",
    "        for i in range(word_len-1):\n",
    "            curr_pair = (k[i], k[i+1])\n",
    "            pair_cnt[curr_pair] += v\n",
    "            pair2keys[curr_pair].add(k)\n",
    "    \n",
    "    # ------ 5. perform merges (modify pair_cnts only)\n",
    "    merges = []\n",
    "    while len(idx2bytes) < vocab_size:        \n",
    "        # ------ 5.1 get candidate with max pair count\n",
    "        max_cnt = max(pair_cnt.values())\n",
    "\n",
    "        if max_cnt == 0:\n",
    "            break\n",
    "\n",
    "        candidates = [k for k, v in pair_cnt.items() if v==max_cnt]\n",
    "        l, r = max(candidates)\n",
    "        curr_merge = l + r\n",
    "        merges.append((l, r))\n",
    "\n",
    "        # ------ 5.2 update keys\n",
    "        keys = deepcopy(pair2keys[(l, r)])\n",
    "        for prev_key in keys:\n",
    "            key_len = len(prev_key)\n",
    "            key_cnt = byte_tokens_cnt.pop(prev_key)\n",
    "\n",
    "            new_key = []\n",
    "            i = 0\n",
    "\n",
    "            while i < key_len:\n",
    "                curr_l = prev_key[i]\n",
    "                if i < key_len - 1:\n",
    "                    curr_r = prev_key[i+1]\n",
    "                else:\n",
    "                    curr_r = None\n",
    "                \n",
    "                if curr_l == l and curr_r == r:\n",
    "                    new_key.append(l+r)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_key.append(curr_l)\n",
    "                    i += 1\n",
    "            \n",
    "            new_key = tuple(new_key)\n",
    "            byte_tokens_cnt[new_key] = key_cnt\n",
    "\n",
    "            # ------ 5.3 update counts and mapping\n",
    "            for left, right in zip(prev_key[:-1], prev_key[1:]):\n",
    "                pair_cnt[(left, right)] -= key_cnt\n",
    "                curr_set = pair2keys[(left, right)]\n",
    "\n",
    "                if prev_key in curr_set:\n",
    "                    curr_set.remove(prev_key)\n",
    "            \n",
    "            for left, right in zip(new_key[:-1], new_key[1:]):\n",
    "                pair_cnt[(left, right)] += key_cnt\n",
    "                pair2keys[(left, right)].add(new_key)\n",
    "            \n",
    "        # ------ 5.4 update idx2bytes\n",
    "        idx2bytes[idx] = curr_merge\n",
    "        idx += 1\n",
    "    \n",
    "    return idx2bytes, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2, merges2 = train_bpe_improved(\n",
    "    \"/data1/ds/scratch/assignment1-basics/data/simple_test.txt\",\n",
    "    10000,\n",
    "    [\"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f98d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe(\n",
    "    input_path: str | os.PathLike,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Initialize Vocabulary\n",
    "    vocab: Dict[int, bytes] = {i: bytes([i]) for i in range(256)}\n",
    "    next_id = 256\n",
    "\n",
    "    special_token_bytes = [token.encode(\"utf-8\") for token in special_tokens]\n",
    "    for token_bytes in special_token_bytes:\n",
    "        if token_bytes not in vocab.values():\n",
    "            vocab[next_id] = token_bytes\n",
    "            next_id += 1\n",
    "\n",
    "    # Step 2: Pre-tokenization\n",
    "    pre_tokens_cnt = defaultdict(int)\n",
    "\n",
    "    def to_bytes_tuple(word: str) -> Tuple[bytes]:\n",
    "        l = list(tuple(word.encode(\"utf-8\")))\n",
    "        l = [bytes([x]) for x in l]\n",
    "        return tuple(l)\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    chunks = re.split(\"|\".join(map(re.escape, special_tokens)), text)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for m in re.finditer(PAT, chunk):\n",
    "            word = m.group(0)\n",
    "            pre_tokens_cnt[to_bytes_tuple(word)] += 1   # key of pre_tokens_cnt e.g. (b'H', b'e', b'l', b'l', b'o')\n",
    "\n",
    "    # Step 3: Compute BPE Merges\n",
    "    merges = []\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        pair_counts = defaultdict(int)\n",
    "\n",
    "        # Count all adjacent byte pairs\n",
    "        for token, cnt in pre_tokens_cnt.items():\n",
    "            for i in range(len(token) - 1):\n",
    "                pair = (token[i], token[i + 1])\n",
    "                pair_counts[pair] += cnt\n",
    "\n",
    "        if not pair_counts:\n",
    "            break  # No more pairs to merge\n",
    "\n",
    "        # Find the most frequent pair(s)\n",
    "        max_count = max(pair_counts.values())\n",
    "        candidates = [k for k, v in pair_counts.items() if v == max_count]\n",
    "        best_pair = max(candidates)\n",
    "\n",
    "        a, b = best_pair\n",
    "\n",
    "        # Create new token\n",
    "        new_token = a + b\n",
    "        vocab[next_id] = new_token\n",
    "        next_id += 1\n",
    "\n",
    "        # Apply the merge to all pre-tokenized sequences\n",
    "        # 收集变更\n",
    "        changes = []\n",
    "        for token, cnt in pre_tokens_cnt.items():\n",
    "            # Find all occurrences of the `best_pair` in `token`\n",
    "            indices = [i for i in range(len(token) - 1) if token[i:i + 2] == best_pair]\n",
    "            if indices:\n",
    "                # Replace each occurrence with `new_token`\n",
    "                new_pre_token = []\n",
    "                i = 0\n",
    "                while i < len(token):\n",
    "                    if i in indices:\n",
    "                        new_pre_token.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_pre_token.append(token[i])\n",
    "                        i += 1\n",
    "                new_pre_token = tuple(new_pre_token)\n",
    "                changes.append((token, new_pre_token, cnt))\n",
    "\n",
    "        # 应用变更\n",
    "        for old_token, new_pre_token, cnt in changes:\n",
    "            pre_tokens_cnt[new_pre_token] = pre_tokens_cnt.get(new_pre_token, 0) + cnt\n",
    "            del pre_tokens_cnt[old_token]\n",
    "\n",
    "        # Record the merge\n",
    "        merges.append((a, b))\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab3, merges3 = run_train_bpe(\n",
    "    \"/data1/ds/scratch/assignment1-basics/data/simple_test.txt\",\n",
    "    10000,\n",
    "    [\"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0623124",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges1 == merges2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16347592",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges2 == merges3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdd1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab1 == vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab3 == vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1babc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce0a2dc",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_tinystories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ts, merges_ts = train_bpe_improved(\n",
    "    \"/data1/ds/scratch/assignment1-basics/data/TinyStoriesV2-GPT4-train.txt\",\n",
    "    10000,\n",
    "    [\"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47862c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_with_pickle(vocab_data, merges_data, filename):\n",
    "    data_to_save = {\n",
    "        \"vocab\": vocab_data,\n",
    "        \"merges\": merges_data\n",
    "    }\n",
    "    \n",
    "    # Open the file in binary write mode ('wb')\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "        \n",
    "    print(f\"Data saved to {filename} using pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed624ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_with_pickle(vocab_ts, merges_ts, \"./TinyStoriesV2-results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001c846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59ec9cd3",
   "metadata": {},
   "source": [
    "### Problem (train_bpe_expts_owt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae4465",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_owt, merges_owt = train_bpe_improved(\n",
    "    \"/data1/ds/scratch/assignment1-basics/data/owt_train.txt\",\n",
    "    32000,\n",
    "    [\"<|endoftext|>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb755740",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_with_pickle(vocab_owt, merges_owt, \"./owt-results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51707c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39406566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ff1d335",
   "metadata": {},
   "source": [
    "## BPE Tokenizer: Encoding and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b53075",
   "metadata": {},
   "source": [
    "### Encoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ca7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11d8c90b",
   "metadata": {},
   "source": [
    "### Decoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab3[256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb6709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1b4e840",
   "metadata": {},
   "source": [
    "#### Problem (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "feaf1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab, merges, special_tokens=None):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "\n",
    "        if special_tokens is None:\n",
    "            special_tokens = []\n",
    "        \n",
    "        self.special_tokens = set(special_tokens)\n",
    "\n",
    "        self.token2idx = {}\n",
    "\n",
    "        for k, v in self.vocab.items():\n",
    "            self.token2idx[v] = k\n",
    "        \n",
    "        self.__add_special_tokens()\n",
    "        # print(self.vocab)\n",
    "        # print(self.merges)\n",
    "        # print(self.special_tokens)\n",
    "        # print(self.token2idx)\n",
    "    \n",
    "\n",
    "    def __add_special_tokens(self):\n",
    "        for token in self.special_tokens:\n",
    "            encoded_token = token.encode(\"utf-8\")\n",
    "\n",
    "            if encoded_token not in self.token2idx:\n",
    "                self.vocab[len(self.vocab)] = encoded_token\n",
    "                self.token2idx[encoded_token] = len(self.token2idx)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath, merges_filepath, filepath, special_tokens=None):\n",
    "        assert vocab_filepath is None or len(vocab_filepath) == 0, f\"Your input vocab_filepath is {vocab_filepath}. You should leave vocab_filepath empty and provide filepath instead\"\n",
    "        assert merges_filepath is None or len(merges_filepath) == 0, f\"Your input merges_filepath is {merges_filepath}. You should leave merges_filepath empty and provide filepath instead\"\n",
    "\n",
    "        try:\n",
    "            with open(filepath, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "        except Exception as exc:\n",
    "            raise\n",
    "        \n",
    "        vocab = data[\"vocab\"]\n",
    "        merges = data[\"merges\"]\n",
    "\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        if len(self.special_tokens) > 0:\n",
    "            escaped = [re.escape(token) for token in self.special_tokens]\n",
    "            pattern = f\"({'|'.join(escaped)})\"\n",
    "            parts = re.split(pattern, text)\n",
    "        else:\n",
    "            parts = [text]\n",
    "\n",
    "        tokens = []\n",
    "        for part in parts:\n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            if part in self.special_tokens:\n",
    "                encoded_part = self.token2idx[part.encode(\"utf-8\")]\n",
    "                tokens.append(encoded_part)\n",
    "            else:\n",
    "                encoded_part = self.encode_helper(part)\n",
    "                tokens.extend(encoded_part)\n",
    "        \n",
    "        return tokens        \n",
    "\n",
    "\n",
    "    def encode_helper(self, text):\n",
    "        GPT2_REGEX_PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        str_tokens = re.findall(GPT2_REGEX_PAT, text)\n",
    "        # print(str_tokens)\n",
    "        unique_str_tokens = {}\n",
    "\n",
    "        for token in str_tokens:\n",
    "            unique_str_tokens[token] = []\n",
    "\n",
    "        for token, _ in unique_str_tokens.items():\n",
    "            encoded = list(token.encode(\"utf-8\")) # [111,222,123,...]\n",
    "            encoded_byte = [bytes([e]) for e in encoded]\n",
    "\n",
    "            pairs = []\n",
    "            pairs_set = set()\n",
    "            for i in range(len(encoded_byte)-1):\n",
    "                pairs.append(encoded_byte[i]+encoded_byte[i+1])\n",
    "                pairs_set.add(encoded_byte[i]+encoded_byte[i+1])\n",
    "            \n",
    "            idx = 0\n",
    "            while len(pairs) >= 1 and idx < len(self.merges):\n",
    "                while idx < len(self.merges):\n",
    "                    curr_merge = self.merges[idx][0] + self.merges[idx][1]\n",
    "                    if curr_merge in pairs_set:\n",
    "                        break\n",
    "                \n",
    "                    idx += 1\n",
    "                \n",
    "                if idx >= len(self.merges):\n",
    "                    break\n",
    "            \n",
    "                curr_pair = self.merges[idx][0] + self.merges[idx][1]\n",
    "                new_encoded_byte = []\n",
    "\n",
    "                i = 0\n",
    "                while i < len(encoded_byte):\n",
    "                    left = encoded_byte[i]\n",
    "                    right = encoded_byte[i+1] if i < len(encoded_byte) - 1 else None\n",
    "\n",
    "                    if right is not None and left + right == curr_pair:\n",
    "                        new_encoded_byte.append(left+right)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_encoded_byte.append(left)\n",
    "                        i += 1\n",
    "                \n",
    "                new_pairs = []\n",
    "                for i in range(len(new_encoded_byte)-1):\n",
    "                    new_pairs.append(new_encoded_byte[i]+new_encoded_byte[i+1])\n",
    "                \n",
    "                pairs = new_pairs\n",
    "                pairs_set = set(pairs)\n",
    "                encoded_byte = new_encoded_byte\n",
    "                \n",
    "                # print(f\"idx={idx}\")\n",
    "                # print(token)\n",
    "                # print(f\"pairs={pairs}\")\n",
    "                # print(f\"encoded_byte={encoded_byte}\")\n",
    "            \n",
    "            for item in encoded_byte:\n",
    "                unique_str_tokens[token].append(self.token2idx[item])\n",
    "        \n",
    "        res = []\n",
    "        for token in str_tokens:\n",
    "            res.extend(unique_str_tokens[token])\n",
    "            \n",
    "        return res\n",
    "\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]):\n",
    "        for string in iterable:\n",
    "            encoded = self.encode(string)\n",
    "            for _id in encoded:\n",
    "                yield _id\n",
    "\n",
    "\n",
    "    def decode(self, ids: list[int]):\n",
    "        decoded = []\n",
    "\n",
    "        for _id in ids:\n",
    "            if _id in self.vocab:\n",
    "                decoded.append(self.vocab[_id])\n",
    "            else:\n",
    "                decoded.append(\"\\uFFFD\".encode(\"utf-8\"))\n",
    "\n",
    "        if len(decoded) == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        res = decoded[0]\n",
    "        for i in range(1, len(decoded)):\n",
    "            res += decoded[i]\n",
    "\n",
    "        return res.decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0945c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88c9447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_bytes_to_unicode() -> dict[int, str]:\n",
    "    # These 188 integers can used as-is, since they are not whitespace or control characters.\n",
    "    # See https://www.ssec.wisc.edu/~tomw/java/unicode.html.\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    cs = bs[:]\n",
    "    # now get the representations of the other 68 integers that do need shifting\n",
    "    # each will get mapped chr(256 + n), where n will grow from 0...67 in the loop\n",
    "    # Get printable representations of the remaining integers 68 integers.\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            # If this integer isn't in our list of visually-representable\n",
    "            # charcters, then map it to the next nice character (offset by 256)\n",
    "            bs.append(b)\n",
    "            cs.append(2**8 + n)\n",
    "            n += 1\n",
    "    characters = [chr(n) for n in cs]\n",
    "    d = dict(zip(bs, characters))\n",
    "    return d\n",
    "\n",
    "def get_tokenizer_from_vocab_merges_path(\n",
    "    vocab_path: str | os.PathLike,\n",
    "    merges_path: str | os.PathLike,\n",
    "    special_tokens: list[str] | None = None,\n",
    "):\n",
    "    import json\n",
    "    gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n",
    "    with open(vocab_path) as vocab_f:\n",
    "        gpt2_vocab = json.load(vocab_f)\n",
    "    gpt2_bpe_merges = []\n",
    "    with open(merges_path) as f:\n",
    "        for line in f:\n",
    "            cleaned_line = line.rstrip()\n",
    "            if cleaned_line and len(cleaned_line.split(\" \")) == 2:\n",
    "                gpt2_bpe_merges.append(tuple(cleaned_line.split(\" \")))\n",
    "    # The GPT-2 tokenizer uses a remapped unicode encoding for bytes. Let's\n",
    "    # just return the original bytes, so we don't force students to use\n",
    "    # any particular encoding scheme.\n",
    "    vocab = {\n",
    "        gpt2_vocab_index: bytes([gpt2_byte_decoder[token] for token in gpt2_vocab_item])\n",
    "        for gpt2_vocab_item, gpt2_vocab_index in gpt2_vocab.items()\n",
    "    }\n",
    "    # If any of the special tokens don't exist in the vocab, append them to the vocab.\n",
    "    if special_tokens:\n",
    "        for special_token in special_tokens:\n",
    "            byte_encoded_special_token = special_token.encode(\"utf-8\")\n",
    "            if byte_encoded_special_token not in set(vocab.values()):\n",
    "                vocab[len(vocab)] = byte_encoded_special_token\n",
    "\n",
    "    merges = [\n",
    "        (\n",
    "            bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n",
    "            bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n",
    "        )\n",
    "        for merge_token_1, merge_token_2 in gpt2_bpe_merges\n",
    "    ]\n",
    "    return Tokenizer(vocab, merges, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "18de9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_PATH = \"/data1/ds/scratch/assignment1-basics/tests/fixtures/gpt2_vocab.json\"\n",
    "MERGES_PATH = \"/data1/ds/scratch/assignment1-basics/tests/fixtures/gpt2_merges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a0f10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer_from_vocab_merges_path(VOCAB_PATH, MERGES_PATH, special_tokens=[\"<|endoftext|>\", \"<|endoftext|><|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8124e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"Hello, how <|endoftext|><|endoftext|> are you?<|endoftext|>\"\n",
    "\n",
    "ids = tokenizer.encode(test_string)\n",
    "tokenized_string = [tokenizer.decode([x]) for x in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "20b104cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " ' how',\n",
       " ' ',\n",
       " '<|endoftext|><|endoftext|>',\n",
       " ' are',\n",
       " ' you',\n",
       " '?',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "87a77753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_string.count(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ceef830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_string.count(\"<|endoftext|><|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cb7d46f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids) == test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262f3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ff8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dell-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
